{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/.local/lib/python3.8/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.0` and `torch==1.10.1+cu113` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import logging\n",
    "from model.simple_stack import SimpleStack\n",
    "gpu_n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the algorithm\n",
    "#### Collect samples\n",
    "At the beginning of each episode (one round of the game), \n",
    "reset the environment to its initial state using `env.reset()`. \n",
    "At each time step ``t``, the environment is at `current_state`.\n",
    "With probability $\\epsilon$, apply a random action.\n",
    "Otherwise, apply $argmax_a~ Q(\\phi($ `current_state` $),a,\\theta)$,\n",
    "where $Q$ is parameterized by paramters $\\theta$ and $\\phi(\\cdot)$ is preprocessor.\n",
    "Pass the action through `env.step(action)` to receive next frame, reward and whether the game terminates.\n",
    "Append this frame to the end of the `current_state` and construct `next_state` while removeing $frame(t-12)$.\n",
    "Store the tuple $(\\phi($ `current_state` $), action, reward, \\phi($ `next_ state` $))$ in the replay buffer.\n",
    "\n",
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Define the following loss\n",
    "$$\\Large(\\small Q(\\phi,a,\\theta)-r-Q(\\phi',argmax_{a'}Q(\\phi',a',\\theta),\\theta^-)\\Large)^2$$\n",
    "* Where $\\theta^-$ is the parameter of the target network.( Set $Q(\\phi',a',\\theta^-)$ to zero if $\\phi$ is the preprocessed termination state). \n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^-$ once in a while\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        #Articheture\n",
    "        self.batch_size = 16 # The size of the batch to learn the Q-function\n",
    "        self.image_size = 84 # Resize the raw input frame to square frame of size 80 by 80 \n",
    "        #Trickes\n",
    "        self.replay_buffer_size = 1000000 # The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "        self.learning_frequency = 4 # With Freq of 1/4 step update the Q-network\n",
    "        self.skip_frame = 4 # Skip 4-1 raw frames between steps\n",
    "        self.internal_skip_frame = 4 # Skip 4-1 raw frames between skipped frames\n",
    "        self.frame_len = 4 # Each state is formed as a concatination 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "        self.Target_update = 10000 # Update the target network each 10000 steps\n",
    "        self.epsilon_min = 0.1 # Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "        self.annealing_end = 1000000. # The number of step it take to linearly anneal the epsilon to it min value\n",
    "        self.gamma = 0.99 # The discount factor\n",
    "        self.replay_start_size = 50000 # Start to backpropagated through the network, learning starts\n",
    "        self.no_op_max = 30 / self.skip_frame # Run uniform policy for first 30 times step of the beginning of the game\n",
    "        \n",
    "        #otimization\n",
    "        self.num_episode = 10000000 # Number episode to run the algorithm\n",
    "        self.max_frame = 200000000\n",
    "        self.lr = 0.00025 # RMSprop learning rate\n",
    "        self.gamma1 = 0.95 # RMSprop gamma1\n",
    "        self.gamma2 = 0.95 # RMSprop gamma2\n",
    "        self.rms_eps = 0.01 # RMSprop epsilon bias\n",
    "        self.ctx = mx.gpu(gpu_n) # Enables gpu if available, if not, set it to mx.cpu()\n",
    "opt = Options()\n",
    "\n",
    "env_name = 'AssaultNoFrameskip-v4' # Set the desired environment\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n # Extract the number of available action from the environment setting\n",
    "\n",
    "logger = logging.getLogger()\n",
    "f_name = './data/results_DDQN_%s.log' %(env_name)\n",
    "fh = logging.handlers.RotatingFileHandler(f_name)\n",
    "fh.setLevel(logging.DEBUG)#no matter what level I set here\n",
    "formatter = logging.Formatter('%(asctime)s:%(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "mx.random.seed(manualSeed)\n",
    "attrs = vars(opt)\n",
    "ff =(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "logging.error(str(ff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DQN model\n",
    "The network is constructed as three CNN layers and a fully connected added on the top. Furthermore, the optimizer is assigned to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = SimpleStack(env.action_space.n, opt.frame_len, channel=channel)\n",
    "dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n",
    "DQN_trainer = gluon.Trainer(dqn.collect_params(),'RMSProp', \\\n",
    "                          {'learning_rate': opt.lr ,'gamma1':opt.gamma1,'gamma2': opt.gamma2,'epsilon': opt.rms_eps,'centered' : True})\n",
    "dqn.collect_params().zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dqn = SimpleStack(env.action_space.n, opt.frame_len, channel=channel)\n",
    "target_dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "Replay buffer store the tuple of : `state`, action , `next_state`, reward , done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'reward','done','initial_state'))\n",
    "class Replay_Buffer():\n",
    "    def __init__(self, replay_buffer_size):\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.replay_buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.replay_buffer_size\n",
    "    def sample(self, batch_size,batch_state,batch_state_next,batch_reward,batch_action,batch_done):\n",
    "        for i in range(batch_size):\n",
    "            j = random.randint(opt.frame_len-1,len(self.memory)-2)\n",
    "            for jj in range(opt.frame_len):\n",
    "                batch_state[i,opt.frame_len-1-jj] =  self.memory[j-jj].state[0].as_in_context(opt.ctx).astype('float32')/255.\n",
    "                batch_state_next[i,opt.frame_len-1-jj] = self.memory[j-jj+1].state.as_in_context(opt.ctx)[0].astype('float32')/255.\n",
    "                if self.memory[j-jj].initial_state:\n",
    "                    for kk in range(opt.frame_len-jj-1):\n",
    "                        batch_state[i,opt.frame_len-2-kk] =  self.memory[j-jj].state[0].as_in_context(opt.ctx).astype('float32')/255.\n",
    "                        batch_state_next[i,opt.frame_len-2-kk] = self.memory[j-jj].state.as_in_context(opt.ctx)[0].astype('float32')/255.\n",
    "                    break\n",
    "            if self.memory[j].done:\n",
    "                batch_state_next[i,opt.frame_len-1] = batch_state[i,opt.frame_len-1]\n",
    "            batch_reward[i] = self.memory[j].reward\n",
    "            batch_action[i] = self.memory[j].action\n",
    "            batch_done[i] = self.memory[j].done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, currentState = None, initial_state = False):\n",
    "    raw_frame = nd.array(raw_frame,mx.cpu())\n",
    "    raw_frame = nd.reshape(nd.mean(raw_frame, axis = 2),shape = (raw_frame.shape[0],raw_frame.shape[1],1))\n",
    "    raw_frame = mx.image.imresize(raw_frame,  opt.image_size, opt.image_size)\n",
    "    raw_frame = nd.transpose(raw_frame, (2,0,1))\n",
    "    raw_frame = raw_frame.astype('float32')/255.\n",
    "    if initial_state == True:\n",
    "        state = raw_frame\n",
    "        for _ in range(opt.frame_len-1):\n",
    "            state = nd.concat(state , raw_frame, dim = 0)\n",
    "    else:\n",
    "        state = mx.nd.concat(currentState[1:,:,:], raw_frame, dim = 0)\n",
    "    return state, raw_frame\n",
    "\n",
    "def rew_clipper(rew):\n",
    "    if rew>0.:\n",
    "        return 1.\n",
    "    elif rew<0.:\n",
    "        return -1.\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def renderimage(next_frame):\n",
    "    if render_image:\n",
    "        plt.imshow(next_frame);\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)\n",
    "        \n",
    "l2loss = gluon.loss.L2Loss(batch_axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_counter = 0. # Counts the number of steps so far\n",
    "annealing_count = 0. # Counts the number of annealing steps\n",
    "epis_count = 0. # Counts the number episodes so far\n",
    "replay_memory = Replay_Buffer(opt.replay_buffer_size) # Initialize the replay buffer\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.00025,epis[200],eps[0.996625],durat[238],fnum=53373, cum_cl_rew = 3, cum_rew = 63,tot_cl = 9 , tot = 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/anaconda/lib/python3.8/site-packages/mxnet/gluon/block.py:463: UserWarning: save_params is deprecated. Please use save_parameters. Note that if you want load from SymbolBlock later, please use export instead. For details, see https://mxnet.apache.org/tutorials/gluon/save_load_params.html\n",
      "  warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n",
      "/home/seventheli/anaconda/lib/python3.8/site-packages/mxnet/gluon/block.py:575: UserWarning: load_params is deprecated. Please use load_parameters.\n",
      "  warnings.warn(\"load_params is deprecated. Please use load_parameters.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.00025,epis[300],eps[0.970301],durat[193],fnum=79697, cum_cl_rew = 5, cum_rew = 105,tot_cl = 8 , tot = 185\n",
      "lr:0.00025,epis[400],eps[0.941899],durat[179],fnum=108099, cum_cl_rew = 5, cum_rew = 105,tot_cl = 9 , tot = 196\n",
      "lr:0.00025,epis[500],eps[0.912083],durat[238],fnum=137915, cum_cl_rew = 5, cum_rew = 105,tot_cl = 9 , tot = 208\n",
      "lr:0.00025,epis[600],eps[0.882118],durat[258],fnum=167880, cum_cl_rew = 9, cum_rew = 189,tot_cl = 9 , tot = 204\n",
      "lr:0.00025,epis[700],eps[0.851956],durat[245],fnum=198042, cum_cl_rew = 9, cum_rew = 189,tot_cl = 9 , tot = 197\n",
      "lr:0.00025,epis[800],eps[0.818734],durat[122],fnum=231264, cum_cl_rew = 4, cum_rew = 84,tot_cl = 10 , tot = 211\n",
      "lr:0.00025,epis[900],eps[0.782779],durat[382],fnum=267219, cum_cl_rew = 12, cum_rew = 252,tot_cl = 10 , tot = 229\n",
      "lr:0.00025,epis[1000],eps[0.747571],durat[240],fnum=302427, cum_cl_rew = 5, cum_rew = 105,tot_cl = 10 , tot = 211\n"
     ]
    }
   ],
   "source": [
    "render_image = False # Whether to render Frames and show the game\n",
    "batch_state = nd.empty((opt.batch_size,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "batch_state_next = nd.empty((opt.batch_size,opt.frame_len,opt.image_size,opt.image_size), opt.ctx)\n",
    "batch_reward = nd.empty((opt.batch_size),opt.ctx)\n",
    "batch_action = nd.empty((opt.batch_size),opt.ctx)\n",
    "batch_done = nd.empty((opt.batch_size),opt.ctx)\n",
    "batch_battery = nd.empty((opt.batch_size, 1),opt.ctx)\n",
    "initial_state = True\n",
    "while epis_count < opt.max_frame:\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    state, current_frame = preprocess(next_frame, initial_state = True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    initial_state = True\n",
    "    while not done:\n",
    "        mx.nd.waitall()\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        renderimage(next_frame)\n",
    "        sample = random.random()\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == opt.replay_start_size:\n",
    "            logging.error('annealing and laerning are started ')\n",
    "        eps = np.maximum(1.-annealing_count/opt.annealing_end,opt.epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < opt.no_op_max:\n",
    "            effective_eps = 1.\n",
    "        # epsilon greedy policy\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = state.reshape([1,opt.frame_len,opt.image_size,opt.image_size]).as_in_context(opt.ctx)\n",
    "            action = int(nd.argmax(dqn([data, nd.array([100], opt.ctx)]),axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        \n",
    "        # Skip frame\n",
    "        rew = 0\n",
    "        for skip in range(opt.skip_frame-1):\n",
    "            next_frame, reward, done,_ = env.step(action)\n",
    "            renderimage(next_frame)\n",
    "            cum_clipped_reward += rew_clipper(reward)\n",
    "            rew += reward\n",
    "            for internal_skip in range(opt.internal_skip_frame-1):\n",
    "                _ , reward, done,_ = env.step(action)\n",
    "                cum_clipped_reward += rew_clipper(reward)\n",
    "                rew += reward\n",
    "                \n",
    "        next_frame_new, reward, done, _ = env.step(action)\n",
    "        renderimage(next_frame)\n",
    "        cum_clipped_reward += rew_clipper(reward)\n",
    "        rew += reward\n",
    "        cum_reward += rew\n",
    "        \n",
    "        # Reward clipping\n",
    "        reward = rew_clipper(rew)\n",
    "        next_frame = np.maximum(next_frame_new,next_frame)\n",
    "        replay_memory.push((current_frame*255.).astype(np.uint8),action,reward,done, initial_state)\n",
    "        state, current_frame = preprocess(next_frame, state)\n",
    "        initial_state = False\n",
    "        # Train\n",
    "        if frame_counter > opt.replay_start_size:        \n",
    "            if frame_counter % opt.learning_frequency == 0:\n",
    "                replay_memory.sample(opt.batch_size,batch_state,batch_state_next,batch_reward,batch_action,batch_done)\n",
    "                with autograd.record():\n",
    "                    argmax_Q = nd.argmax(dqn([batch_state_next, batch_battery]),axis = 1).astype('uint8')\n",
    "                    Q_sp = nd.pick(target_dqn([batch_state_next, batch_battery]),argmax_Q,1)\n",
    "                    Q_sp = Q_sp*(nd.ones(opt.batch_size,ctx = opt.ctx)-batch_done)\n",
    "                    Q_s_array = dqn([batch_state, batch_battery])\n",
    "                    Q_s = nd.pick(Q_s_array,batch_action,1)\n",
    "                    loss = nd.mean(l2loss(Q_s, (batch_reward + opt.gamma *Q_sp)))\n",
    "                loss.backward()\n",
    "                DQN_trainer.step(opt.batch_size)\n",
    "        \n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Save the model and update Target model\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            if frame_counter % opt.Target_update == 0 :\n",
    "                check_point = frame_counter / (opt.Target_update *100)\n",
    "                fdqn = './data/target_%s_%d' % (env_name,int(check_point))\n",
    "                dqn.save_params(fdqn)\n",
    "                target_dqn.load_params(fdqn, opt.ctx)\n",
    "                fnam = './data/clippted_rew_DDQN%s_lr_%f' %(env_name,opt.lr)\n",
    "                np.save(fnam,tot_clipped_reward)\n",
    "                fnam = './data/tot_rew_DDQN%s_lr_%f' %(env_name,opt.lr)\n",
    "                np.save(fnam,tot_reward)\n",
    "                fnam = './data/frame_count_DDQN%s_lr_%f' %(env_name,opt.lr)\n",
    "                np.save(fnam,frame_count_record) \n",
    "                \n",
    "        if done:\n",
    "            if epis_count % 100. == 0. :\n",
    "                print('lr:%s,epis[%d],eps[%f],durat[%d],fnum=%d, cum_cl_rew = %d, cum_rew = %d,tot_cl = %d , tot = %d'\\\n",
    "                  %(opt.lr,epis_count,eps,t+1,frame_counter,cum_clipped_reward,cum_reward,moving_average_clipped,moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record,frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count)-1-100:int(epis_count)-1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count)-1-100:int(epis_count)-1])\n",
    "from tempfile import TemporaryFile\n",
    "outfile = TemporaryFile()\n",
    "outfile_clip = TemporaryFile()\n",
    "np.save(outfile, moving_average)\n",
    "np.save(outfile_clip, moving_average_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epis_count = epis_count-0\n",
    "bandwidth = 100 # Moving average bandwidth\n",
    "total_clipped = np.zeros(int(num_epis_count)-bandwidth)\n",
    "total_rew = np.zeros(int(num_epis_count)-bandwidth)\n",
    "for i in range(int(num_epis_count)-bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i+bandwidth])/bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i+bandwidth])/bandwidth\n",
    "t = np.arange(int(num_epis_count)-bandwidth)\n",
    "fig = plt.figure()\n",
    "belplt = plt.plot(t,total_rew[0:int(num_epis_count)-bandwidth],\"r\", label = \"Return\")\n",
    "plt.legend()#handles[likplt,belplt])\n",
    "print('Running after %d number of episodes' %epis_count)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Assualt_DDQN.png')\n",
    "fig = plt.figure()\n",
    "likplt = plt.plot(t,total_clipped[0:opt.num_episode-bandwidth],\"b\", label = \"Clipped Return\")\n",
    "plt.legend()#handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average clipped Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Assualt_DDQN_Clipped.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated average reward after 8000 episodes of game Assault\n",
    "|![](./Assualt_DDQN.png)|![](./Assualt_DDQN_Clipped.png)|\n",
    "|:---------------:|:---------------:|\n",
    "|Average reward|Average clipped reward|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}