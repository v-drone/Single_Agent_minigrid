{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from model import SimpleStack\n",
    "from utils import check_dir\n",
    "from memory import Memory\n",
    "from algorithm.DQN import DQN\n",
    "from environments.SimpleEnv import SimpleEnv\n",
    "from utils import copy_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent view\n",
    "agent_view = 7\n",
    "map_size = 10\n",
    "# action max\n",
    "action_max = 3\n",
    "# learning rate\n",
    "model_save = \"./model_save/\"\n",
    "lr = 0.002\n",
    "# start play\n",
    "replay_start_size = 20000\n",
    "# update step\n",
    "update_step = 1000\n",
    "# gamma in q-loss calculation\n",
    "gamma = 0.99\n",
    "# memory pool size\n",
    "memory_length = 500000\n",
    "# file to save train log\n",
    "summary = \"./test_{}\".format(str(time.time()))\n",
    "# the number of step it take to linearly anneal the epsilon to it min value\n",
    "annealing_end = 1000000\n",
    "# min level of stochastically of policy (epsilon)-greedy\n",
    "epsilon_min = 0.2\n",
    "# temporary files\n",
    "temporary_model = \"./{}/model.params\".format(model_save)\n",
    "temporary_pool = \"./{}/pool\".format(model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(summary):\n",
    "    os.remove(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "for i in [\"model_save\", \"data_save\"]:\n",
    "    check_dir(i)\n",
    "# build models\n",
    "online_model = SimpleStack(agent_view, map_size)\n",
    "offline_model = SimpleStack(agent_view, map_size)\n",
    "online_model.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "offline_model.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "offline_model.collect_params().zero_grad()\n",
    "print(\"create model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "online_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model = \"./phase1.params\"\n",
    "# if os.path.exists(temporary_model):\n",
    "#     online_model.load_parameters(load_model, ctx=ctx)\n",
    "#     offline_model.load_parameters(load_model, ctx=ctx)\n",
    "#     print(\"load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleEnv(display=False)\n",
    "env.reset_env()\n",
    "# create pool\n",
    "memory_pool = Memory(memory_length)\n",
    "algorithm = DQN([online_model, offline_model], ctx, lr, gamma, memory_pool, action_max, temporary_model,bz=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish = 0\n",
    "all_step_counter = 0\n",
    "annealing_count = 0\n",
    "cost = []\n",
    "texts = []\n",
    "num_episode = 500000\n",
    "tot_reward = np.zeros(num_episode)\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_counter = 0\n",
    "tmp_reward = 0\n",
    "for epoch in range(_epoch, num_episode):\n",
    "    _epoch += 1\n",
    "    env.reset_env()\n",
    "    finish = 0\n",
    "    cum_clipped_reward = 0\n",
    "    while not finish:\n",
    "        if all_step_counter > replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if all_step_counter == replay_start_size:\n",
    "            print('annealing and learning are started')\n",
    "        eps = np.maximum(1 - all_step_counter / annealing_end, epsilon_min)\n",
    "        action, by = algorithm.get_action(env.state(), eps)\n",
    "        old, new, reward_get, finish, original_reward = env.step(action)\n",
    "        memory_pool.add(old, new, action, reward_get, finish)\n",
    "        cum_clipped_reward += original_reward\n",
    "        all_step_counter += 1\n",
    "        if finish and len(env.finish) > 50:\n",
    "            sr_50 = sum(env.finish[-50:]) / min(len(env.finish), 50)\n",
    "            ar_50 = sum(env.total_reward[-50:]) / sum(env.total_step_count[-50:])\n",
    "            sr_all = sum(env.finish) / len(env.finish)\n",
    "            ar_all = sum(env.total_reward) / sum(env.total_step_count)\n",
    "            text = \"success rate last 50 %f, avg return %f; success rate total %f, avg return total %f\" % (\n",
    "                sr_50, ar_50, sr_all, ar_all)\n",
    "            with open(summary, \"a\") as f:\n",
    "                f.writelines(text + \"\\n\")\n",
    "            if epoch % 100 == 0:\n",
    "                print(text + \"; %f\" % eps)\n",
    "        # save model and replace online model each epoch\n",
    "        if annealing_count > replay_start_size and annealing_count % update_step == 0:\n",
    "            copy_params(offline_model, online_model)\n",
    "            offline_model.save_parameters(temporary_model)\n",
    "    #  train every 4 epoch\n",
    "    if annealing_count > replay_start_size and epoch % 4 == 0:\n",
    "        cost.append(algorithm.train())\n",
    "    tot_reward[int(epoch) - 1] = cum_clipped_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_number = \"1\"\n",
    "bandwidth = 1000 # Moving average bandwidth\n",
    "total_rew = np.zeros(int(_epoch)-bandwidth)\n",
    "for i in range(int(_epoch)-bandwidth):\n",
    "    total_rew[i] = np.sum(tot_reward[i:i+bandwidth])/bandwidth\n",
    "t = np.arange(int(_epoch)-bandwidth)\n",
    "belplt = plt.plot(t,total_rew[0:int(_epoch)-bandwidth],\"r\", label = \"Return\")\n",
    "#handles[belplt])\n",
    "plt.legend()\n",
    "print('Running after %d number of episodes' %_epoch)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.savefig(\"R5_train_%s.jpg\" % _number)\n",
    "plt.show()\n",
    "np.save(\"R5_train_%s.array\" % _number, env.total_step_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}