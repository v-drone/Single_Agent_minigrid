{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/.local/lib/python3.8/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.0` and `torch==1.10.1+cu113` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import time\n",
    "import logging\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "from IPython import display\n",
    "from memory import Memory\n",
    "from utils import preprocess\n",
    "from model.simple_stack import SimpleStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the algorithm\n",
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Define the following loss\n",
    "$$\\Large(\\small Q(\\phi,a,\\theta)-r-Q(\\phi',argmax_{a'}Q(\\phi',a',\\theta),\\theta^-)\\Large)^2$$\n",
    "* Where $\\theta^-$ is the parameter of the target network.( Set $Q(\\phi',a',\\theta^-)$ to zero if $\\phi$ is the preprocessed termination state). \n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^-$ once in a while\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# frame channel\n",
    "channel = 4\n",
    "# The size of the batch to learn the Q-function\n",
    "batch_size = 32\n",
    "# Resize the raw input frame to square frame of size 80 by 80\n",
    "image_size = 84\n",
    "# The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "replay_buffer_size = 1000000\n",
    "# With Freq of 1/N step update the Q-network\n",
    "learning_frequency = 4\n",
    "# Skip 4-1 raw frames between steps\n",
    "skip_frame = 4\n",
    "# Skip 4-1 raw frames between skipped frames\n",
    "internal_skip_frame = 4\n",
    "# Each state is formed as a concatenation 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "frame_len = 4\n",
    "# Update the target network each 10000 steps\n",
    "target_update = 10000\n",
    "# Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "epsilon_min = 0.1\n",
    "# The number of step it take to linearly anneal the epsilon to it min value\n",
    "annealing_end = 1000000.\n",
    "# The discount factor\n",
    "gamma = 0.99\n",
    "# Start to back propagated through the network, learning starts\n",
    "replay_start_size = 50000\n",
    "# Run uniform policy for first 30 times step of the beginning of the game\n",
    "no_op_max = 30 / skip_frame\n",
    "# Number episode to run the algorithm\n",
    "num_episode = 10000000\n",
    "max_frame = 200000000\n",
    "# RMSprop learning rate\n",
    "lr = 0.001\n",
    "# RMSprop gamma1\n",
    "gamma1 = 0.95\n",
    "# RMSprop gamma2\n",
    "gamma2 = 0.95\n",
    "# RMSprop epsilon bias\n",
    "rms_eps = 0.01\n",
    "# Enables gpu if available, if not, set it to mx.cpu()\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env = gym.make(env_name)\n",
    "num_action = 3\n",
    "manualSeed = 1\n",
    "mx.random.seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = SimpleStack(num_action, frame_len, channel=channel)\n",
    "dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "trainer = gluon.Trainer(dqn.collect_params(), 'RMSProp',\n",
    "                        {'learning_rate': lr, 'gamma1': gamma1, 'gamma2': gamma2, 'epsilon': rms_eps, 'centered': True})\n",
    "dqn.collect_params().zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dqn = SimpleStack(num_action, frame_len, channel=channel)\n",
    "target_dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_image(frame, render):\n",
    "    if render:\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = mx.gluon.loss.L2Loss(batch_axis=0)\n",
    "# Counts the number of steps so far\n",
    "frame_counter = 0.\n",
    "# Counts the number of annealing steps\n",
    "annealing_count = 0.\n",
    "# Counts the number episodes so far\n",
    "epis_count = 0.\n",
    "# Initialize the replay buffer\n",
    "replay_memory = Memory(replay_buffer_size)\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, image_size, channel=4, frame_len=4, current_state=None, initial_state=False):\n",
    "    raw_frame = nd.array(raw_frame, mx.cpu())\n",
    "    if channel == 1:\n",
    "        raw_frame = nd.max(raw_frame, axis=2)\n",
    "        raw_frame = nd.reshape(raw_frame, shape=(raw_frame.shape[0], raw_frame.shape[1], 1))\n",
    "    raw_frame = mx.image.imresize(raw_frame, image_size, image_size)\n",
    "    raw_frame = nd.transpose(raw_frame, (2, 0, 1))\n",
    "    raw_frame = raw_frame.astype(np.float32) / 255\n",
    "    if initial_state:\n",
    "        state = raw_frame\n",
    "        for _ in range(frame_len - 1):\n",
    "            state = nd.concat(state, raw_frame, dim=0)\n",
    "    else:\n",
    "        state = mx.nd.concat(current_state[raw_frame.shape[0]:, :, :], raw_frame, dim=0)\n",
    "    return state, raw_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6fd34bdc6c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m        \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_frame\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m        \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m        \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m        \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m        \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/gym_minigrid/minigrid.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close, highlight, tile_size)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m                 \u001b[0;31m# Compute the world coordinates of this cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m                 \u001b[0mabs_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_left\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf_vec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvis_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr_vec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvis_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mabs_i\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mabs_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Whether to render Frames and show the game\n",
    "_render = False\n",
    "batch_state = nd.empty((batch_size, frame_len, image_size, image_size), ctx)\n",
    "batch_state_next = nd.empty((batch_size, frame_len, image_size, image_size), ctx)\n",
    "batch_reward = nd.empty([batch_size], ctx)\n",
    "batch_action = nd.empty([batch_size], ctx)\n",
    "batch_done = nd.empty([batch_size], ctx)\n",
    "batch_battery = nd.empty((batch_size, 100), ctx)\n",
    "while epis_count < max_frame:\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    next_frame = env.render(False)\n",
    "    state, current_frame = preprocess(next_frame, image_size, channel, frame_len, initial_state=True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    initial_state = True\n",
    "    while not done:\n",
    "        mx.nd.waitall()\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        render_image(current_frame.transpose([1, 2, 0]).asnumpy() /255, _render)\n",
    "        sample = random.random()\n",
    "        if frame_counter > replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == replay_start_size:\n",
    "            print('annealing and learning are started ')\n",
    "        eps = np.maximum(1. - annealing_count / annealing_end, epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < no_op_max:\n",
    "            effective_eps = 1.\n",
    "        # epsilon greedy policy\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = [nd.array(state.reshape([1, frame_len, image_size, image_size]), ctx), nd.array([100 - t], ctx)]\n",
    "            action = int(nd.argmax(dqn(data), axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        next_frame_new, reward, done, _ = env.step(action)\n",
    "        render_image(current_frame.transpose([1, 2, 0]).asnumpy() /255, _render)\n",
    "        cum_clipped_reward += reward\n",
    "        cum_reward += reward\n",
    "        # Reward clipping\n",
    "        # reward = rew_clipper(rew)\n",
    "        # End Reward clipping\n",
    "        replay_memory.push((current_frame * 255.).astype(np.uint8), action, reward, done, 100 -t, initial_state)\n",
    "        state, current_frame = preprocess(next_frame, image_size, channel, frame_len, current_state=state)\n",
    "        _, current_frame = preprocess(env.render(False), image_size, channel, frame_len, initial_state=True)\n",
    "        initial_state = False\n",
    "        # Train\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % learning_frequency == 0:\n",
    "                replay_memory.sample(batch_size, batch_state, batch_state_next, batch_reward, batch_action, batch_done,batch_battery)\n",
    "                with autograd.record():\n",
    "                    argmax_Q = nd.argmax(dqn([batch_state_next, batch_battery]), axis=1).astype('uint8')\n",
    "                    Q_sp = nd.pick(target_dqn([batch_state_next, batch_battery]), argmax_Q, 1)\n",
    "                    Q_sp = Q_sp * (nd.ones(batch_size, ctx=ctx) - batch_done)\n",
    "                    Q_s_array = dqn([batch_state, batch_battery])\n",
    "                    Q_s = nd.pick(Q_s_array, batch_action, 1)\n",
    "                    loss = nd.mean(loss_f(Q_s, (batch_reward + gamma * Q_sp)))\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        # Save the model and update Target model\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % target_update == 0:\n",
    "                check_point = frame_counter / (target_update * 100)\n",
    "                file_name = './data/target_%s_%d' % (env_name, int(check_point))\n",
    "                dqn.save_parameters(file_name)\n",
    "                target_dqn.load_parameters(file_name, ctx)\n",
    "                reward_name = './data/clippted_rew_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, tot_clipped_reward)\n",
    "                reward_name = './data/tot_rew_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, tot_reward)\n",
    "                reward_name = './data/frame_count_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, frame_count_record)\n",
    "        if done:\n",
    "            if epis_count % 100. == 0.:\n",
    "                print(\"lr:%s,epis[%d],eps[%f],durat[%d],fnum=%d, cum_cl_rew = %.4f, cum_rew = %.4f,tot_cl = %.4f, tot = %.4f\"\n",
    "                      % (lr, epis_count, eps, t, frame_counter, cum_clipped_reward, cum_reward,\n",
    "                         moving_average_clipped, moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record, frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "outfile = TemporaryFile()\n",
    "outfile_clip = TemporaryFile()\n",
    "np.save(outfile, moving_average)\n",
    "np.save(outfile_clip, moving_average_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epis_count = epis_count - 0\n",
    "bandwidth = 100  # Moving average bandwidth\n",
    "total_clipped = np.zeros(int(num_epis_count) - bandwidth)\n",
    "total_rew = np.zeros(int(num_epis_count) - bandwidth)\n",
    "for i in range(int(num_epis_count) - bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i + bandwidth]) / bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i + bandwidth]) / bandwidth\n",
    "t = np.arange(int(num_epis_count) - bandwidth)\n",
    "fig = plt.figure()\n",
    "belplt = plt.plot(t, total_rew[0:int(num_epis_count) - bandwidth], \"r\", label=\"Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "print('Running after %d number of episodes' % epis_count)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Assualt_DDQN.png')\n",
    "fig = plt.figure()\n",
    "likplt = plt.plot(t, total_clipped[0:num_episode - bandwidth], \"b\", label=\"Clipped Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average clipped Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('DDQN_Clipped.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated average reward after 8000 episodes of game Assault\n",
    "|![](./Assualt_DDQN.png)|![](./Assualt_DDQN_Clipped.png)|\n",
    "|:---------------:|:---------------:|\n",
    "|Average reward|Average clipped reward|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
