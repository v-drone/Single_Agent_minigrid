{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/anaconda3/lib/python3.8/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.8.0` and `torch==1.10.1+cu113` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import time\n",
    "import logging\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "from IPython import display\n",
    "from memory import Memory\n",
    "from utils import preprocess\n",
    "from model.simple_stack import SimpleStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the algorithm\n",
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Define the following loss\n",
    "$$\\Large(\\small Q(\\phi,a,\\theta)-r-Q(\\phi',argmax_{a'}Q(\\phi',a',\\theta),\\theta^-)\\Large)^2$$\n",
    "* Where $\\theta^-$ is the parameter of the target network.( Set $Q(\\phi',a',\\theta^-)$ to zero if $\\phi$ is the preprocessed termination state). \n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^-$ once in a while\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# frame channel\n",
    "channel = 3\n",
    "# The size of the batch to learn the Q-function\n",
    "batch_size = 64\n",
    "# Resize the raw input frame to square frame of size 80 by 80\n",
    "image_size = 128\n",
    "# The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "replay_buffer_size = 50000\n",
    "# With Freq of 1/N step update the Q-network\n",
    "learning_frequency = 4\n",
    "# Each state is formed as a concatenation 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "frame_len = 1\n",
    "# Update the target network each 10000 steps\n",
    "target_update = 10000\n",
    "# Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "epsilon_min = 0.01\n",
    "# The number of step it take to linearly anneal the epsilon to it min value\n",
    "annealing_end = 100000.\n",
    "# The discount factor\n",
    "gamma = 0.99\n",
    "# Start to back propagated through the network, learning starts\n",
    "replay_start_size = 50000\n",
    "# Run uniform policy for first N times step of the beginning of the game\n",
    "no_op_max = 5\n",
    "# Number episode to run the algorithm\n",
    "num_episode = 10000000\n",
    "max_frame = 200000000\n",
    "# RMSprop learning rate\n",
    "lr = 0.0025\n",
    "# RMSprop gamma1\n",
    "gamma1 = 0.95\n",
    "# RMSprop gamma2\n",
    "gamma2 = 0.95\n",
    "# RMSprop epsilon bias\n",
    "rms_eps = 0.01\n",
    "# Enables gpu if available, if not, set it to mx.cpu()\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env = gym.make(env_name)\n",
    "num_action = 3\n",
    "manualSeed = 1\n",
    "mx.random.seed(manualSeed)\n",
    "env_name = env_name + \"-Map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = SimpleStack(num_action, frame_len, channel=channel)\n",
    "dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "trainer = gluon.Trainer(dqn.collect_params(), 'RMSProp',\n",
    "                        {'learning_rate': lr, 'gamma1': gamma1, 'gamma2': gamma2, 'epsilon': rms_eps, 'centered': True})\n",
    "dqn.collect_params().zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dqn = SimpleStack(num_action, frame_len, channel=channel)\n",
    "target_dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_image(frame, render):\n",
    "    if render:\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = mx.gluon.loss.L2Loss(batch_axis=0)\n",
    "# Counts the number of steps so far\n",
    "frame_counter = 0.\n",
    "# Counts the number of annealing steps\n",
    "annealing_count = 0.\n",
    "# Counts the number episodes so far\n",
    "epis_count = 0.\n",
    "# Initialize the replay buffer\n",
    "replay_memory = Memory(replay_buffer_size, frame_len, channel)\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "t_record = []\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, image_size, channel=3, frame_len=4, current_state=None, initial_state=False):\n",
    "    raw_frame = nd.array(raw_frame, mx.cpu())\n",
    "    if channel == 1:\n",
    "        raw_frame = nd.max(raw_frame, axis=2)\n",
    "        raw_frame = nd.reshape(raw_frame, shape=(raw_frame.shape[0], raw_frame.shape[1], 1))\n",
    "    raw_frame = mx.image.imresize(raw_frame, image_size, image_size)\n",
    "    raw_frame = nd.transpose(raw_frame, (2, 0, 1))\n",
    "    raw_frame = raw_frame.astype(np.float32) / 255\n",
    "    if frame_len > 1:\n",
    "        if initial_state:\n",
    "            state = raw_frame\n",
    "            for _ in range(frame_len - 1):\n",
    "                state = nd.concat(state, raw_frame, dim=0)\n",
    "        else:\n",
    "            state = mx.nd.concat(current_state[raw_frame.shape[0]:, :, :], raw_frame, dim=0)\n",
    "        return state, raw_frame\n",
    "    else:\n",
    "        return raw_frame, raw_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rew_clipper(row, history):\n",
    "    counter = 0\n",
    "    _ = list(reversed(history))[0]\n",
    "    for i in list(reversed(history))[1:]:\n",
    "        if i == _:\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "    return (row- 0.0001 * counter) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_image(preprocess(env.render(False), image_size, channel, frame_len, initial_state=True)[0][-3:].transpose([1,2,0]).asnumpy(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.0025,epis[0],eps[1.000000],durat[144],fnum=144,tot_cl = 0.0000, tot = 0.0000\n",
      "lr:0.0025,epis[50],eps[1.000000],durat[77],fnum=3943,tot_cl = 0.0000, tot = 0.0000\n",
      "lr:0.0025,epis[100],eps[1.000000],durat[74],fnum=7477,tot_cl = 0.0000, tot = 0.0000\n",
      "lr:0.0025,epis[150],eps[1.000000],durat[75],fnum=11354,tot_cl = 47.5499, tot = 0.5046\n",
      "lr:0.0025,epis[200],eps[1.000000],durat[76],fnum=15344,tot_cl = 44.7224, tot = 0.4811\n",
      "lr:0.0025,epis[250],eps[1.000000],durat[78],fnum=19619,tot_cl = 42.0423, tot = 0.4569\n",
      "lr:0.0025,epis[300],eps[1.000000],durat[77],fnum=23215,tot_cl = 43.9635, tot = 0.4751\n"
     ]
    }
   ],
   "source": [
    " # Whether to render Frames and show the game\n",
    "_render = False\n",
    "batch_state = nd.empty((batch_size, frame_len * channel, image_size, image_size), ctx)\n",
    "batch_state_next = nd.empty((batch_size, frame_len * channel, image_size, image_size), ctx)\n",
    "batch_reward = nd.empty([batch_size], ctx)\n",
    "batch_action = nd.empty([batch_size], ctx)\n",
    "batch_done = nd.empty([batch_size], ctx)\n",
    "batch_battery = nd.empty((batch_size, 100), ctx)\n",
    "while epis_count < max_frame:\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    env.reset()\n",
    "    next_frame = env.env.render(False)\n",
    "    state, current_frame = preprocess(next_frame, image_size, channel, frame_len, initial_state=True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    location = [env.env.agent_pos.tolist()]\n",
    "    while not done:\n",
    "        mx.nd.waitall()\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        render_image(current_frame.asnumpy().transpose([1,2,0]), _render)\n",
    "        sample = random.random()\n",
    "        if frame_counter > replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == replay_start_size:\n",
    "            print(\"annealing and learning are started tot = %.4f\" % moving_average)\n",
    "        eps = np.maximum(1. - annealing_count / annealing_end, epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < no_op_max:\n",
    "            effective_eps = 1.\n",
    "        # epsilon greedy policy\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = [nd.array(state.reshape([1, frame_len * channel, image_size, image_size]), ctx), nd.array([100 - t], ctx)]\n",
    "            _ = dqn(data)\n",
    "            # print(_.asnumpy().tolist())\n",
    "            action = int(nd.argmax(_, axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        \n",
    "        next_frame, reward, done, _ = env.step(action)\n",
    "        location.append(env.env.agent_pos.tolist())\n",
    "\n",
    "        cum_reward += reward\n",
    "        \n",
    "        # Reward clipping\n",
    "        reward = rew_clipper(reward, location)\n",
    "        cum_clipped_reward += reward\n",
    "        # End Reward clipping\n",
    "        \n",
    "        # _ = env.env.get_obs_render(env.env.gen_obs()[\"image\"])\n",
    "        _ = env.env.render(False)\n",
    "        \n",
    "        state, current_frame = preprocess(_, image_size, channel, frame_len, initial_state=False)        \n",
    "        replay_memory.push(previous_state, action,  state, reward, done, 100 - t)\n",
    "        render_image(current_frame.asnumpy().transpose([1,2,0]), _render)\n",
    "        # Train\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % learning_frequency == 0:\n",
    "                data = replay_memory.sample(batch_size, ctx)\n",
    "                batch_state = nd.empty((batch_size, frame_len * channel, image_size, image_size), ctx)\n",
    "                with autograd.record():\n",
    "                    argmax_Q = nd.argmax(dqn([data.state_next, data.battery]),axis = 1).astype('uint8')\n",
    "                    Q_sp = nd.pick(target_dqn([data.state_next, data.battery]),argmax_Q,1)\n",
    "                    Q_sp = Q_sp*(nd.ones(batch_size,ctx = ctx)-data.finish)\n",
    "                    Q_s_array = dqn([data.state, data.battery])\n",
    "                    Q_s = nd.pick(Q_s_array,data.action,1)\n",
    "                    loss = nd.mean(loss_f(Q_s ,  (data.reward.reshape(batch_size) + gamma *Q_sp)))\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        # Save the model and update Target model\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % target_update == 0:\n",
    "                check_point = frame_counter / (target_update * 100)\n",
    "                file_name = './data/target_%s_%d' % (env_name, int(check_point))\n",
    "                dqn.save_parameters(file_name)\n",
    "                target_dqn.load_parameters(file_name, ctx)\n",
    "                reward_name = './data/tot_rew_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, tot_reward)\n",
    "                reward_name = './data/frame_count_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, frame_count_record)\n",
    "        if done:\n",
    "            t_record.append(t)\n",
    "            if epis_count % 50. == 0.:\n",
    "                print(\"lr:%s,epis[%d],eps[%f],durat[%d],fnum=%d,tot_cl = %.4f, tot = %.4f\"\n",
    "                      % (lr, epis_count, eps, np.mean(t_record), frame_counter, moving_average_clipped, moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record, frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "outfile = TemporaryFile()\n",
    "outfile_clip = TemporaryFile()\n",
    "np.save(outfile, moving_average)\n",
    "np.save(outfile_clip, moving_average_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([i.state_next.shape  for i in replay_memory.memory])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epis_count = epis_count - 0\n",
    "bandwidth = 100  # Moving average bandwidth\n",
    "total_clipped = np.zeros(int(num_epis_count) - bandwidth)\n",
    "total_rew = np.zeros(int(num_epis_count) - bandwidth)\n",
    "for i in range(int(num_epis_count) - bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i + bandwidth]) / bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i + bandwidth]) / bandwidth\n",
    "t = np.arange(int(num_epis_count) - bandwidth)\n",
    "fig = plt.figure()\n",
    "belplt = plt.plot(t, total_rew[0:int(num_epis_count) - bandwidth], \"r\", label=\"Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "print('Running after %d number of episodes' % epis_count)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Grid_DDQN.png')\n",
    "fig = plt.figure()\n",
    "likplt = plt.plot(t, total_clipped[0:num_episode - bandwidth], \"b\", label=\"Clipped Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average clipped Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Grid_Clipped.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
