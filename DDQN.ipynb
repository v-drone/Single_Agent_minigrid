{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/.local/lib/python3.8/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.0` and `torch==1.10.1+cu113` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import gym\n",
    "import time\n",
    "import logging\n",
    "from memory import Memory\n",
    "from utils import preprocess, rew_clipper\n",
    "from model.simple_stack import SimpleStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the algorithm\n",
    "#### Collect samples\n",
    "At the beginning of each episode (one round of the game), \n",
    "reset the environment to its initial state using `env.reset()`. \n",
    "At each time step ``t``, the environment is at `current_state`.\n",
    "With probability $\\epsilon$, apply a random action.\n",
    "Otherwise, apply $argmax_a~ Q(\\phi($ `current_state` $),a,\\theta)$,\n",
    "where $Q$ is parameterized by paramters $\\theta$ and $\\phi(\\cdot)$ is preprocessor.\n",
    "Pass the action through `env.step(action)` to receive next frame, reward and whether the game terminates.\n",
    "Append this frame to the end of the `current_state` and construct `next_state` while removeing $frame(t-12)$.\n",
    "Store the tuple $(\\phi($ `current_state` $), action, reward, \\phi($ `next_ state` $))$ in the replay buffer.\n",
    "\n",
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Define the following loss\n",
    "$$\\Large(\\small Q(\\phi,a,\\theta)-r-Q(\\phi',argmax_{a'}Q(\\phi',a',\\theta),\\theta^-)\\Large)^2$$\n",
    "* Where $\\theta^-$ is the parameter of the target network.( Set $Q(\\phi',a',\\theta^-)$ to zero if $\\phi$ is the preprocessed termination state). \n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^-$ once in a while\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# frame channel\n",
    "channel = 1\n",
    "# The size of the batch to learn the Q-function\n",
    "batch_size = 16\n",
    "# Resize the raw input frame to square frame of size 80 by 80\n",
    "image_size = 84\n",
    "# The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "replay_buffer_size = 1000000\n",
    "# With Freq of 1/N step update the Q-network\n",
    "learning_frequency = 4\n",
    "# Skip 4-1 raw frames between steps\n",
    "skip_frame = 4\n",
    "# Skip 4-1 raw frames between skipped frames\n",
    "internal_skip_frame = 4\n",
    "# Each state is formed as a concatenation 4 step frames [f(t-12),f(t-8),f(t-4),f(t)]\n",
    "frame_len = 4\n",
    "# Update the target network each 10000 steps\n",
    "target_update = 10000\n",
    "# Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "epsilon_min = 0.1\n",
    "# The number of step it take to linearly anneal the epsilon to it min value\n",
    "annealing_end = 1000000.\n",
    "# The discount factor\n",
    "gamma = 0.99\n",
    "# Start to back propagated through the network, learning starts\n",
    "replay_start_size = 50000\n",
    "# Run uniform policy for first 30 times step of the beginning of the game\n",
    "no_op_max = 30 / skip_frame\n",
    "# Number episode to run the algorithm\n",
    "num_episode = 10000000\n",
    "max_frame = 200000000\n",
    "# RMSprop learning rate\n",
    "lr = 0.00025\n",
    "# RMSprop gamma1\n",
    "gamma1 = 0.95\n",
    "# RMSprop gamma2\n",
    "gamma2 = 0.95\n",
    "# RMSprop epsilon bias\n",
    "rms_eps = 0.01\n",
    "# Enables gpu if available, if not, set it to mx.cpu()\n",
    "ctx = mx.gpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AssaultNoFrameskip-v4'\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n\n",
    "manualSeed = 1\n",
    "mx.random.seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = SimpleStack(env.action_space.n, frame_len, channel=channel)\n",
    "dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "trainer = gluon.Trainer(dqn.collect_params(), 'RMSProp',\n",
    "                        {'learning_rate': lr, 'gamma1': gamma1, 'gamma2': gamma2, 'epsilon': rms_eps, 'centered': True})\n",
    "dqn.collect_params().zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dqn = SimpleStack(env.action_space.n, frame_len, channel=channel)\n",
    "target_dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_image(frame, render):\n",
    "    if render:\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = mx.gluon.loss.L2Loss(batch_axis=0)\n",
    "# Counts the number of steps so far\n",
    "frame_counter = 0.\n",
    "# Counts the number of annealing steps\n",
    "annealing_count = 0.\n",
    "# Counts the number episodes so far\n",
    "epis_count = 0.\n",
    "# Initialize the replay buffer\n",
    "replay_memory = Memory(replay_buffer_size)\n",
    "tot_clipped_reward = []\n",
    "tot_reward = []\n",
    "frame_count_record = []\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.00025,epis[200],eps[0.996625],durat[238],fnum=53373, cum_cl_rew = 3, cum_rew = 63,tot_cl = 9 , tot = 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/anaconda/lib/python3.8/site-packages/mxnet/gluon/block.py:463: UserWarning: save_params is deprecated. Please use save_parameters. Note that if you want load from SymbolBlock later, please use export instead. For details, see https://mxnet.apache.org/tutorials/gluon/save_load_params.html\n",
      "  warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n",
      "/home/seventheli/anaconda/lib/python3.8/site-packages/mxnet/gluon/block.py:575: UserWarning: load_params is deprecated. Please use load_parameters.\n",
      "  warnings.warn(\"load_params is deprecated. Please use load_parameters.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.00025,epis[300],eps[0.970301],durat[193],fnum=79697, cum_cl_rew = 5, cum_rew = 105,tot_cl = 8 , tot = 185\n",
      "lr:0.00025,epis[400],eps[0.941899],durat[179],fnum=108099, cum_cl_rew = 5, cum_rew = 105,tot_cl = 9 , tot = 196\n",
      "lr:0.00025,epis[500],eps[0.912083],durat[238],fnum=137915, cum_cl_rew = 5, cum_rew = 105,tot_cl = 9 , tot = 208\n"
     ]
    }
   ],
   "source": [
    " # Whether to render Frames and show the game\n",
    "_render = False\n",
    "batch_state = nd.empty((batch_size, frame_len, image_size, image_size), ctx)\n",
    "batch_state_next = nd.empty((batch_size, frame_len, image_size, image_size), ctx)\n",
    "batch_reward = nd.empty([batch_size], ctx)\n",
    "batch_action = nd.empty([batch_size], ctx)\n",
    "batch_done = nd.empty([batch_size], ctx)\n",
    "batch_battery = nd.empty((batch_size, 1), ctx)\n",
    "while epis_count < max_frame:\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    state, current_frame = preprocess(next_frame, image_size, channel, frame_len, initial_state=True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    initial_state = True\n",
    "    while not done:\n",
    "        mx.nd.waitall()\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        render_image(next_frame, _render)\n",
    "        sample = random.random()\n",
    "        if frame_counter > replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == replay_start_size:\n",
    "            logging.error('annealing and learning are started ')\n",
    "        eps = np.maximum(1. - annealing_count / annealing_end, epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < no_op_max:\n",
    "            effective_eps = 1.\n",
    "        # epsilon greedy policy\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = [nd.array(state.reshape([1, frame_len, image_size, image_size]), ctx), nd.array([100], ctx)]\n",
    "            action = int(nd.argmax(dqn([data, nd.array([100], ctx)]), axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        # Skip frame\n",
    "        rew = 0\n",
    "        for skip in range(skip_frame - 1):\n",
    "            next_frame, reward, done, _ = env.step(action)\n",
    "            render_image(next_frame, _render)\n",
    "            cum_clipped_reward += rew_clipper(reward)\n",
    "            rew += reward\n",
    "            for internal_skip in range(internal_skip_frame - 1):\n",
    "                _, reward, done, _ = env.step(action)\n",
    "                cum_clipped_reward += rew_clipper(reward)\n",
    "                rew += reward\n",
    "\n",
    "        next_frame_new, reward, done, _ = env.step(action)\n",
    "        render_image(next_frame, _render)\n",
    "        cum_clipped_reward += rew_clipper(reward)\n",
    "        rew += reward\n",
    "        cum_reward += rew\n",
    "\n",
    "        # Reward clipping\n",
    "        reward = rew_clipper(rew)\n",
    "        next_frame = np.maximum(next_frame_new, next_frame)\n",
    "        replay_memory.push((current_frame * 255.).astype(np.uint8), action, reward, done, initial_state)\n",
    "        state, current_frame = preprocess(next_frame, state)\n",
    "        initial_state = False\n",
    "        # Train\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % learning_frequency == 0:\n",
    "                replay_memory.sample(batch_size, batch_state, batch_state_next, batch_reward, batch_action, batch_done)\n",
    "                with autograd.record():\n",
    "                    argmax_Q = nd.argmax(dqn([batch_state_next, batch_battery]), axis=1).astype('uint8')\n",
    "                    Q_sp = nd.pick(target_dqn([batch_state_next, batch_battery]), argmax_Q, 1)\n",
    "                    Q_sp = Q_sp * (nd.ones(batch_size, ctx=ctx) - batch_done)\n",
    "                    Q_s_array = dqn([batch_state, batch_battery])\n",
    "                    Q_s = nd.pick(Q_s_array, batch_action, 1)\n",
    "                    loss = nd.mean(loss_f(Q_s, (batch_reward + gamma * Q_sp)))\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        # Save the model and update Target model\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % target_update == 0:\n",
    "                check_point = frame_counter / (target_update * 100)\n",
    "                file_name = './data/target_%s_%d' % (env_name, int(check_point))\n",
    "                dqn.save_params(file_name)\n",
    "                target_dqn.load_params(file_name, ctx)\n",
    "                reward_name = './data/clippted_rew_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, tot_clipped_reward)\n",
    "                reward_name = './data/tot_rew_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, tot_reward)\n",
    "                reward_name = './data/frame_count_DDQN%s_lr_%f' % (env_name, lr)\n",
    "                np.save(reward_name, frame_count_record)\n",
    "\n",
    "        if done:\n",
    "            if epis_count % 100. == 0.:\n",
    "                print('lr:%s,epis[%d],eps[%f],durat[%d],fnum=%d, cum_cl_rew = %d, cum_rew = %d,tot_cl = %d , tot = %d'\n",
    "                      % (lr, epis_count, eps, t + 1, frame_counter, cum_clipped_reward, cum_reward,\n",
    "                         moving_average_clipped, moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record, frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "outfile = TemporaryFile()\n",
    "outfile_clip = TemporaryFile()\n",
    "np.save(outfile, moving_average)\n",
    "np.save(outfile_clip, moving_average_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epis_count = epis_count - 0\n",
    "bandwidth = 100  # Moving average bandwidth\n",
    "total_clipped = np.zeros(int(num_epis_count) - bandwidth)\n",
    "total_rew = np.zeros(int(num_epis_count) - bandwidth)\n",
    "for i in range(int(num_epis_count) - bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i + bandwidth]) / bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i + bandwidth]) / bandwidth\n",
    "t = np.arange(int(num_epis_count) - bandwidth)\n",
    "fig = plt.figure()\n",
    "belplt = plt.plot(t, total_rew[0:int(num_epis_count) - bandwidth], \"r\", label=\"Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "print('Running after %d number of episodes' % epis_count)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Assualt_DDQN.png')\n",
    "fig = plt.figure()\n",
    "likplt = plt.plot(t, total_clipped[0:num_episode - bandwidth], \"b\", label=\"Clipped Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average clipped Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('DDQN_Clipped.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated average reward after 8000 episodes of game Assault\n",
    "|![](./Assualt_DDQN.png)|![](./Assualt_DDQN_Clipped.png)|\n",
    "|:---------------:|:---------------:|\n",
    "|Average reward|Average clipped reward|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}