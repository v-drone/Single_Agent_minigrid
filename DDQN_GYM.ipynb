{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seventheli/.local/lib/python3.8/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.0` and `torch==1.10.0+cu113` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from memory import Memory\n",
    "from mxnet import nd, autograd\n",
    "from IPython import display\n",
    "from collections import namedtuple\n",
    "from model.simple_stack import SimpleStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Network\n",
    "* Draw batches of tuples from the replay buffer: $(\\phi,r,a,\\phi')$.\n",
    "* Define the following loss\n",
    "$$\\Large(\\small Q(\\phi,a,\\theta)-r-Q(\\phi',argmax_{a'}Q(\\phi',a',\\theta),\\theta^-)\\Large)^2$$\n",
    "* Where $\\theta^-$ is the parameter of the target network.( Set $Q(\\phi',a',\\theta^-)$ to zero if $\\phi$ is the preprocessed termination state). \n",
    "* Update the $\\theta$\n",
    "* Update the $\\theta^-$ once in a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Articheture\n",
    "channel = 1\n",
    "batch_size = 32\n",
    "image_size = 84\n",
    "#Trickes\n",
    "# The size of replay buffer; set it to size of your memory (.5M for 50G available memory)\n",
    "replay_buffer_size = 100000\n",
    "# With Freq of 1/4 step update the Q-network\n",
    "learning_frequency = 4\n",
    "# Skip N-1 raw frames between steps\n",
    "skip_frame = 1\n",
    " # Skip N-1 raw frames between skipped frames\n",
    "internal_skip_frame = 1\n",
    "# Each state is formed as a concatination N step frames [f(t-4),f(t-3),f(t-2),f(t)]\n",
    "frame_len = 4\n",
    "# Update the target network each 10000 steps\n",
    "Target_update = 10000\n",
    "# Minimum level of stochasticity of policy (epsilon)-greedy\n",
    "epsilon_min = 0.1\n",
    "# The number of step it take to linearly anneal the epsilon to it min value\n",
    "annealing_end = 1000000.\n",
    "# The discount factor\n",
    "gamma = 0.99\n",
    "# Start to backpropagated through the network, learning starts\n",
    "replay_start_size = 5000\n",
    "# Run uniform policy for first 5 times step of the beginning of the game\n",
    "no_op_max = 10 / skip_frame\n",
    "#otimization\n",
    "num_episode = 10000000  # Number episode to run the algorithm\n",
    "max_frame = 200000000\n",
    "# RMSprop learning rate\n",
    "lr = 0.001\n",
    "# RMSprop gamma1\n",
    "gamma1 = 0.95  \n",
    "# RMSprop gamma2\n",
    "gamma2 = 0.95\n",
    "rms_eps = 0.01\n",
    "# Enables gpu if available, if not, set it to mx.cpu()\n",
    "ctx = mx.gpu()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AssaultNoFrameskip-v4'\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n\n",
    "manualSeed = 1  # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "mx.random.seed(manualSeed)\n",
    "loss_f = mx.gluon.loss.L2Loss(batch_axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "mx.random.seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_model = SimpleStack(env.action_space.n, frame_len, channel=channel)\n",
    "offline_model.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "offline_model.collect_params().zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_model = SimpleStack(env.action_space.n, frame_len, channel=channel)\n",
    "online_model.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = mx.gluon.Trainer(offline_model.collect_params(), 'RMSProp',\n",
    "                           {'learning_rate': lr, 'gamma1': gamma1, 'gamma2': gamma2, 'epsilon': rms_eps, 'centered': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, current_state=None, initial_state=False):\n",
    "    raw_frame = nd.array(raw_frame, mx.cpu())\n",
    "    if channel == 1:\n",
    "        raw_frame = nd.mean(raw_frame, axis=2)\n",
    "    raw_frame = nd.reshape(raw_frame, shape=(raw_frame.shape[0], raw_frame.shape[1], channel))\n",
    "    raw_frame = mx.image.imresize(raw_frame, image_size, image_size)\n",
    "    raw_frame = nd.transpose(raw_frame, (2, 0, 1))\n",
    "    raw_frame = raw_frame.astype(np.float32) / 255\n",
    "    if initial_state:\n",
    "        state = raw_frame\n",
    "        for _ in range(frame_len - 1):\n",
    "            state = nd.concat(state, raw_frame, dim=0)\n",
    "    else:\n",
    "        state = mx.nd.concat(current_state[channel:, :, :], raw_frame, dim=0)\n",
    "    return state, raw_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rew_clipper(_):\n",
    "    if _ > 0.:\n",
    "        return 1.\n",
    "    elif _ < 0.:\n",
    "        return -1.\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderimage(next_frame):\n",
    "    if render_image:\n",
    "        plt.imshow(next_frame)\n",
    "        plt.show()\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of steps so far\n",
    "frame_counter = 0.\n",
    "# Counts the number of annealing steps\n",
    "annealing_count = 0.\n",
    "# Counts the number episodes so far\n",
    "epis_count = 0.\n",
    "# Initialize the replay buffer\n",
    "replay_memory = Memory(replay_buffer_size, frame_len=frame_len)\n",
    "tot_reward = []\n",
    "tot_clipped_reward = []\n",
    "frame_count_record = []\n",
    "average_clipped = 0.\n",
    "average = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_image = False  # Whether to render Frames and show the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_state = nd.empty((batch_size,frame_len,image_size,image_size), ctx)\n",
    "batch_state_next = nd.empty((batch_size,frame_len,image_size,image_size), ctx)\n",
    "batch_reward = nd.empty((batch_size), ctx)\n",
    "batch_action = nd.empty((batch_size), ctx)\n",
    "batch_done = nd.empty((batch_size), ctx)\n",
    "batch_battery = nd.empty((batch_size), ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis[0],eps[1.000000],durat[1881],fnum=1881, cum_cl_rew = 12.000000, cum_rew = 252.000000,tot_cl = 0.000000 , tot = 0.000000\n",
      "annealing and laerning are started \n"
     ]
    }
   ],
   "source": [
    "while frame_counter < max_frame:\n",
    "    initial_state = True\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    state, current_frame = preprocess(next_frame, initial_state=initial_state)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    while not done:\n",
    "        mx.nd.waitall()\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        renderimage(next_frame)\n",
    "        sample = random.random()\n",
    "        if frame_counter > replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == replay_start_size:\n",
    "            print('annealing and laerning are started ')\n",
    "        eps = np.maximum(1. - annealing_count / annealing_end, epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < no_op_max:\n",
    "            effective_eps = 1.\n",
    "        # epsilon greedy policy\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = [nd.array(state.reshape([1, frame_len, image_size, image_size]), ctx), nd.array([100], ctx)]\n",
    "            action = int(nd.argmax(offline_model(data), axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        # Skip frame\n",
    "        frame_rewards = 0\n",
    "        for skip in range(skip_frame-1):\n",
    "            next_frame, reward, done, _ = env.step(action)\n",
    "            renderimage(next_frame)\n",
    "            cum_clipped_reward += rew_clipper(reward)\n",
    "            rew += reward\n",
    "            for internal_skip in range(internal_skip_frame-1):\n",
    "                _ , reward, done,_ = env.step(action)\n",
    "                cum_clipped_reward += rew_clipper(reward)\n",
    "                frame_rewards += reward\n",
    "        next_frame, reward, done, extra = env.step(action)\n",
    "        extra = extra[\"lives\"]\n",
    "        renderimage(next_frame)\n",
    "        cum_clipped_reward += rew_clipper(reward)\n",
    "        frame_rewards += reward\n",
    "        cum_reward += frame_rewards\n",
    "        # Reward clipping\n",
    "        reward = rew_clipper(frame_rewards)\n",
    "        replay_memory.push((current_frame * 255.).astype(np.uint8), action, reward, done, extra, initial_state)\n",
    "        state, current_frame = preprocess(next_frame, state)\n",
    "        initial_state = False\n",
    "        # Train\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % learning_frequency == 0:\n",
    "                replay_memory.sample(batch_size,batch_state,batch_state_next,batch_reward,batch_action,batch_done, batch_battery)\n",
    "                with autograd.record():\n",
    "                    argmax_Q = nd.argmax(offline_model([batch_state_next, batch_battery]), axis=1).astype('uint8')\n",
    "                    Q_sp = nd.pick(online_model([batch_state_next, batch_battery]), argmax_Q, 1)\n",
    "                    Q_sp = Q_sp * (nd.ones(batch_size, ctx=ctx) - batch_done)\n",
    "                    Q_s_array = offline_model([batch_state, batch_battery])\n",
    "                    Q_s = nd.pick(Q_s_array, batch_action, 1)\n",
    "                    loss = nd.mean(loss_f(Q_s, (batch_reward + gamma * Q_sp)))\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        # Save the model and update Target model\n",
    "        if frame_counter > replay_start_size:\n",
    "            if frame_counter % Target_update == 0:\n",
    "                check_point = frame_counter / (Target_update * 100)\n",
    "                fdqn = './data/model_%s_%d' % (env_name, int(check_point))\n",
    "                offline_model.save_parameters(fdqn)\n",
    "                online_model.load_parameters(fdqn, ctx)\n",
    "                fnam = './data/clippted_rew_DDQN_%s' % (env_name)\n",
    "                np.save(fnam, tot_clipped_reward)\n",
    "                fnam = './data/tot_rew_DDQN_%s' % (env_name)\n",
    "                np.save(fnam, tot_reward)\n",
    "                fnam = './data/frame_count_DDQN_%s' % (env_name)\n",
    "                np.save(fnam, frame_count_record)\n",
    "\n",
    "        if done:\n",
    "            if epis_count % 100 == 0.:\n",
    "                print('epis[%d],eps[%f],durat[%d],fnum=%d, cum_cl_rew = %f, cum_rew = %f,tot_cl = %f , tot = %f'\n",
    "                      % (epis_count, eps, t, frame_counter, cum_clipped_reward, cum_reward, average_clipped,average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward = np.append(tot_clipped_reward, cum_clipped_reward)\n",
    "    tot_reward = np.append(tot_reward, cum_reward)\n",
    "    frame_count_record = np.append(frame_count_record, frame_counter)\n",
    "    if epis_count > 100.:\n",
    "        average_clipped = np.mean(tot_clipped_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "        average = np.mean(tot_reward[int(epis_count) - 1 - 100:int(epis_count) - 1])\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "outfile = TemporaryFile()\n",
    "outfile_clip = TemporaryFile()\n",
    "np.save(outfile, moving_average)\n",
    "np.save(outfile_clip, average_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('frame', 'action', 'reward', 'finish', 'battery', 'initial'))\n",
    "Transition((current_frame * 255.).astype(np.uint8), action, reward, done, extra, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the overall performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epis_count = epis_count - 0\n",
    "bandwidth = 1000  # Moving average bandwidth \n",
    "total_clipped = np.zeros(int(num_epis_count) - bandwidth)\n",
    "total_rew = np.zeros(int(num_epis_count) - bandwidth)\n",
    "for i in range(int(num_epis_count) - bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i + bandwidth]) / bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i + bandwidth]) / bandwidth\n",
    "t = np.arange(int(num_epis_count) - bandwidth)\n",
    "fig = plt.figure()\n",
    "belplt = plt.plot(t, total_rew[0:int(num_epis_count) - bandwidth], \"r\", label=\"Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "print('Running after %d number of episodes' % epis_count)\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Assualt_DDQN.png')\n",
    "fig = plt.figure()\n",
    "likplt = plt.plot(t, total_clipped[0:num_episode - bandwidth], \"b\", label=\"Clipped Return\")\n",
    "plt.legend()  #handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of episode\")\n",
    "plt.ylabel(\"Average clipped Reward per episode\")\n",
    "plt.show()\n",
    "fig.savefig('Assualt_DDQN_Clipped.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
